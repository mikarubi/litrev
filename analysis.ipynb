{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import tkinter\n",
    "import zipfile\n",
    "import textwrap\n",
    "try:\n",
    "    import requests\n",
    "    import html2text\n",
    "except ModuleNotFoundError as err:\n",
    "    print('Warning: %s.'%err)\n",
    "import ipywidgets as iw\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "# set output directory and file names\n",
    "out_dir = os.path.join('.', 'results')\n",
    "refs_dir = os.path.join(out_dir, 'refs')\n",
    "eqns_dir = os.path.join(out_dir, 'eqns')\n",
    "results_file = os.path.join(out_dir, 'results.zip')\n",
    "summary_file = os.path.join(out_dir, 'summary.csv')\n",
    "records_file = os.path.join('.', 'savedrecs.txt')\n",
    "\n",
    "# check if archive file exists and create output directories\n",
    "processing_complete = os.path.isfile(results_file)\n",
    "[os.makedirs(di, exist_ok=True) for di in [refs_dir, eqns_dir]]\n",
    "\n",
    "# configure http header, ignore insecure request warnings and test connection status\n",
    "hdr = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Gecko/20100101 Firefox/60.0'}\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "try:\n",
    "    requests.get('https://www.google.com/', verify=False)\n",
    "    online = True\n",
    "except requests.ConnectionError:\n",
    "    print('Connection Error: Proceeding in offline mode.')\n",
    "    online = False\n",
    "\n",
    "# configure box alignment\n",
    "align_center = iw.Layout(align_items='center')\n",
    "    \n",
    "print('Environment setup complete.')\n",
    "\n",
    "if processing_complete:\n",
    "    def make_repr(button):\n",
    "        global processing_complete\n",
    "        processing_complete = 0\n",
    "        print('Allowing to reprocess results.')\n",
    "                    \n",
    "    print('All processing is complete. Extracting data... ', end='')\n",
    "    with zipfile.ZipFile(results_file, 'r') as zf:\n",
    "        for fi in zf.namelist():\n",
    "            if fi == 'records.json':\n",
    "                with zf.open(fi, 'r') as f:\n",
    "                    recs = json.load(f)\n",
    "            else:\n",
    "                zf.extract(fi, eqns_dir)\n",
    "    print('done.')\n",
    "    \n",
    "    processing_complete_message = iw.HTML('''\n",
    "        <div class=\"alert alert-block alert-info\" style=\"font-weight: 600\">\n",
    "        Processing is complete &mdash; run step 6 to view results.<br></div>\n",
    "        ''')\n",
    "    \n",
    "    repr_button = iw.Button(description='Allow to reprocess results',\n",
    "                            layout=iw.Layout(\n",
    "                                    width='200px',\n",
    "                                    style={'description_width': 'initial'}))\n",
    "    repr_button.on_click(make_repr)\n",
    "    \n",
    "    display(iw.VBox([processing_complete_message, repr_button], layout=align_center))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Download reference records from Web of Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if processing_complete:\n",
    "    display(iw.VBox([processing_complete_message], layout=align_center))\n",
    "\n",
    "# define query\n",
    "query = '''\n",
    "((TS=(\"network neuroscience\")) OR ((TS=(\"connectom*\")) AND (TS=(\"analy*\") OR\n",
    "TS=(\"model*\"))) OR ((TS=(\"*brain*\") OR TS=(\"*cort*\")) AND (TS=(\"network theor*\") OR\n",
    "TS=(\"network analy*\") OR TS=(\"network topolog*\") OR TS=(\"network control*\") OR\n",
    "TS=(\"graph theor*\") OR TS=(\"complex network*\")))) AND (SO=(\"nature\") OR\n",
    "SO=(\"science\") OR SO=(\"nature communications\") OR\n",
    "SO=(\"proceedings of the national academy of sciences of the united states of america\")\n",
    "OR SO=(\"nature neuroscience\") OR SO=(\"neuron\") OR SO=(\"elife\") OR\n",
    "SO=(\"plos biology\") OR SO=(\"brain\") OR SO=(\"biological psychiatry\")) AND\n",
    "(PY=(\"2014\") OR PY=(\"2015\") OR PY=(\"2016\") OR PY=(\"2017\") OR PY=(\"2018\")) AND\n",
    "(DT=(\"article\"))\n",
    "'''.replace('\\n', ' ')\n",
    "\n",
    "# format structured version of query\n",
    "query_structured = indent = ''\n",
    "for i, qi in enumerate(query):\n",
    "        qs = qi\n",
    "        qg, qh = query[i-2:i-1], query[i-1:i]\n",
    "        qj, qk = query[i+1:i+2], query[i+2:i+3]\n",
    "        if (qi=='(' and not qj=='\"') or (qh+qi=='OR') or (qg+qh+qi=='AND'):\n",
    "            indent += ('  ' if qi=='(' else '')\n",
    "            qs = qs+'\\n'+(indent[:-1] if qj==' ' else indent)\n",
    "        if (qi==')' and not qh=='\"') or (qi+qj=='OR') or (qi+qj+qk=='AND'):\n",
    "            indent = (indent[:-2] if qi==')' else indent) \n",
    "            qs = '\\n'+indent+qs\n",
    "        query_structured += qs\n",
    "        \n",
    "# format compressed version of query\n",
    "query_compressed = '\"'.join([qi.replace(' ', '_') if i%2 else qi for i, qi in enumerate(query.split('\"'))])\n",
    "query_compressed = textwrap.fill(query_compressed, width=84, break_long_words=False, break_on_hyphens=False)\n",
    "query_compressed = '\"'.join([qi.replace('_', ' ') if i%2 else qi for i, qi in enumerate(query_compressed.split('\"'))])\n",
    "\n",
    "# define titles and values of tabs\n",
    "substeps = [\n",
    "['Step 2A', (\n",
    "    'Navigate to the [webofknowledge.com](http://www.webofknowledge.com) website and '\n",
    "    'click on the _Advanced Search_ link. ![screenshot](screenshots/step_2a.png)')],\n",
    "['Search query', []],\n",
    "['Step 2B', (\n",
    "    'Paste the _`Search query`_ into the text area and press the _Search_ button.'\n",
    "    '![screenshot](screenshots/step_2b.png)')],\n",
    "['Step 2C', (\n",
    "    'Click on the number in the _Results_ column to view the results.'\n",
    "    '![screenshot](screenshots/step_2c.png)')],\n",
    "['Step 2D', (\n",
    "    'Choose the _Save to Other File Formats_ option from the main drop-down menu.'\n",
    "    '![screenshot](screenshots/step_2d.png)')],\n",
    "['Step 2E','''\n",
    "Export all references:\n",
    "\n",
    "- Select the _Records_ radio button and enter the values `1` and _`total number of records`_ in the text fields.\n",
    "- Choose the _Author, Title, Source, Abstract_ option from the _Record Content_ drop-down menu.\n",
    "- Choose the _Plain Text_ option from the _File Format_ drop-down menu.\n",
    "- Press the _Send_ button and save the output to: `%s`\n",
    "\n",
    "![screenshot](screenshots/step_2e.png)\n",
    "'''%os.path.abspath(records_file)]]\n",
    "\n",
    "# create query widget\n",
    "query_menu_structured = iw.Output()\n",
    "query_menu_compressed = iw.Output()\n",
    "query_menu_structured.append_display_data(Markdown('<pre>'+query_structured+'</pre>'))\n",
    "query_menu_compressed.append_display_data(Markdown('<pre>'+query_compressed+'</pre>'))\n",
    "query_menu = iw.Tab(children=[query_menu_structured, query_menu_compressed])\n",
    "    \n",
    "# create menu widgets and visualize\n",
    "range_l = range(len(substeps))\n",
    "menu_children = [query_menu if i==1 else iw.Output() for i in range_l]\n",
    "menu = iw.Tab(children=menu_children)\n",
    "for i, si in enumerate(substeps):\n",
    "    menu.set_title(i, si[0])\n",
    "    if i != 1:\n",
    "        menu.children[i].append_display_data(Markdown(si[1]))\n",
    "    else:\n",
    "        menu.children[i].set_title(0, 'Structured format')\n",
    "        menu.children[i].set_title(1, 'Compressed format')\n",
    "        \n",
    "display(menu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Parse reference records and get article text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if processing_complete:\n",
    "    display(iw.VBox([processing_complete_message], layout=align_center))\n",
    "    \n",
    "else:\n",
    "    # get reference metadata\n",
    "    try:\n",
    "        with open(records_file, 'r') as f:\n",
    "            recs = f.readlines()\n",
    "    except:\n",
    "        display(iw.VBox([iw.HTML('''\n",
    "                        <div class=\"alert alert-block alert-danger\" style=\"font-weight: 600\">\n",
    "                        Cannot open %s<br>\n",
    "                        Make sure to download this file as described in the previous cell.</div>\n",
    "                        '''%os.path.abspath(records_file))], layout=align_center))\n",
    "\n",
    "    # separate metadata by record\n",
    "    recs = (''.join(recs[2:-2])).split('\\n\\n')\n",
    "    recs = [ri.strip().split('\\n') for ri in recs]\n",
    "\n",
    "    # convert metadata to dictionaries\n",
    "    for i, ri in enumerate(recs):\n",
    "        ri = ['<<<'+li[:2]+'>>>'+li[2:] if li[0].isalpha() else li for li in ri]\n",
    "        ri = ('\\n'.join(ri)).split('<<<')[1:]\n",
    "        ri = [li.split('>>>') for li in ri]\n",
    "        recs[i] = OrderedDict(ri)\n",
    "        \n",
    "    # remove duplicates (records with matching dois)\n",
    "    dupl = [i for i, ri in enumerate(recs) for rj in recs[i+1:] if ri['DI'] == rj['DI']]\n",
    "    recs = [ri for i, ri in enumerate(recs) if i not in dupl]\n",
    "\n",
    "    # loop over doi of references\n",
    "    for i, ri in enumerate(recs):\n",
    "        # reset variables\n",
    "        pid = doi = acc = ref = jnl = url = None\n",
    "        \n",
    "        # try to get doi from pubmed (more reliable)\n",
    "        try:\n",
    "            pid = ri['PM'].strip()\n",
    "            idc = 'https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?ids='+pid\n",
    "            xml = requests.get(idc, headers=hdr, verify=False).content.decode()\n",
    "            doi = re.findall(r'(?s)doi=\"(.*?)\"', xml)[0]\n",
    "        # get doi directly from record if pubmed fails\n",
    "        except (KeyError, IndexError):\n",
    "            doi = ri['DI'].strip()\n",
    "            \n",
    "        # define accession number, ref filename and journal name\n",
    "        acc = ri['UT'].split(':')[1].strip()\n",
    "        ref = os.path.join(refs_dir, 'ref%s.htm'%acc)\n",
    "        jnl = ' '.join(ri['SO'].upper().replace('\\n', ' ').split())\n",
    "\n",
    "        # get url from pii if elsevier\n",
    "        if jnl in {'BIOLOGICAL PSYCHIATRY', 'NEURON'} and online:\n",
    "            aid = requests.get('https://dx.doi.org/'+doi, verify=False).url.split('/')[-1]\n",
    "            pii = aid[:5]+'-'+aid[5:9]+'('+aid[9:11]+')'+aid[11:16]+'-'+aid[16]\n",
    "            if jnl == 'BIOLOGICAL PSYCHIATRY':\n",
    "                url = 'https://www.biologicalpsychiatryjournal.com/article/'+pii+'/fulltext'\n",
    "            elif jnl == 'NEURON':\n",
    "                url = 'https://www.cell.com/neuron/fulltext/'+pii\n",
    "\n",
    "        # get url from doi otherwise\n",
    "        else:\n",
    "            url = 'https://dx.doi.org/'+doi\n",
    "        \n",
    "        # download if article doesn't exist\n",
    "        print('Article %d/%d'%(i+1, len(recs)))\n",
    "        if not os.path.isfile(ref):\n",
    "            print('Saving to ref: '+ref)\n",
    "\n",
    "            # download, convert to markdown, and save\n",
    "            get = requests.get(url, headers=hdr, verify=False)\n",
    "            with open(ref, 'wb') as f:\n",
    "                f.write(get.content)\n",
    "\n",
    "        # store accession number, journal name and html\n",
    "        recs[i]['UT'] = acc\n",
    "        recs[i]['SO'] = jnl\n",
    "        with open(ref, 'rb') as f:\n",
    "            recs[i]['HTM'] = f.read().decode()\n",
    "\n",
    "    print('\\n\\nArticle text loaded.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Clean article text formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if processing_complete:\n",
    "    display(iw.VBox([processing_complete_message], layout=align_center))\n",
    "\n",
    "else:\n",
    "    # define auxiliary functions\n",
    "    notitle = lambda si: re.sub(r'(?s)title=(\".*?\"|\\'.*?\\')', '', si)\n",
    "    dat2src = lambda si: si.replace('data-src', 'src')\n",
    "    img2url = lambda si, jnl: '!'+si if isimg(si, jnl) else si\n",
    "    isimg = lambda si, jnl: (si[1:5]=='Math') or (si[1]==']' and jnl not in {'ELIFE', 'SCIENCE'})\n",
    "    natrhdg = lambda si: si.replace('## ', '### ')\n",
    "\n",
    "    # html to markdown conversion\n",
    "    def htm2txt(si):\n",
    "        text_maker = html2text.HTML2Text()\n",
    "        text_maker.body_width = 0\n",
    "        text_maker.ignore_emphasis = True\n",
    "        text_maker.use_automatic_links = False\n",
    "        return text_maker.handle(si)\n",
    "\n",
    "    for i, ri in enumerate(recs):\n",
    "        # get journal name and text\n",
    "        print('Article %d/%d'%(i+1, len(recs)))\n",
    "        jnl = ri['SO']\n",
    "        txt = ri['HTM']\n",
    "\n",
    "        # remove tables, remove link titles and fix image tags\n",
    "        txt = re.sub(r'(?s)<table.*?</table>', ' ', txt)\n",
    "        txt = re.sub(r'(?s)(<a(.*?)>)', lambda fi: notitle(fi.group()), txt)\n",
    "        txt = re.sub(r'(?s)<img.*?>', lambda fi: dat2src(fi.group()), txt)\n",
    "\n",
    "        # protect headings and math formatting\n",
    "        txt = txt.replace('</h2>','</h2>\\n\\n')\n",
    "        txt = re.sub(r'(?s)(</){,1}mml:', '\\g<1>', txt)\n",
    "        txt = re.sub(r'(?s)<math.*?>', '<math>', txt)\n",
    "\n",
    "        # convert to markdown\n",
    "        if '<math>' in txt:\n",
    "            # extract math formatting\n",
    "            txt = txt.split('<math>')\n",
    "            t0 = txt.pop(0)\n",
    "            txtm, txt = zip(*(ti.split('</math>') for ti in txt))\n",
    "            txtm = [('<math>'+ti+'</math>').replace('>', '> ') for ti in txtm]\n",
    "            txt = htm2txt('[math]'.join([t0] + list(txt)))\n",
    "\n",
    "            # recover math formatting\n",
    "            txt = txt.split('[math]')\n",
    "            t0 = txt.pop(0)\n",
    "            txt = ''.join([t0] + [''.join(ti) for ti in zip(txtm, txt)])\n",
    "        else:\n",
    "            txt = htm2txt(txt)\n",
    "\n",
    "        # remove inline images except equations\n",
    "        txt = re.sub(r'(?s)\\!(\\[.*?\\]\\(.*?\\))', lambda fi: img2url(fi.group(1), jnl), txt)\n",
    "\n",
    "        # format equations\n",
    "        eqn_style = 'alt=\"Equation\" style=\"display:inline-block; vertical-align:bottom\"'\n",
    "        txt = re.sub(r'(?s)\\!\\[.*?\\]\\((.*?)\\)', '<img src=\"\\g<1>\" '+eqn_style+'>', txt)\n",
    "\n",
    "        # remove all links\n",
    "        txt = txt.split('[')\n",
    "        t0 = txt.pop(0)\n",
    "        txt = [re.sub(r'(?s)\\]\\(.*?\\)', ']', ti) for ti in txt]\n",
    "        txt = '['.join([t0] + txt)\n",
    "\n",
    "        # journal-specific formatting fixes\n",
    "        if jnl == 'BRAIN':\n",
    "            # remove duplicate elements\n",
    "            idx = [fi.start() for fi in re.finditer(r'(?s)\\n\\n(?:Figure|Table)\\s[0-9]{1,2}\\n\\n', txt)]\n",
    "            dbl = [txt[i:j] for i, j in zip(idx[:-1],idx[1:]) if txt[i:j] in txt[j:]]\n",
    "            for di in dbl:\n",
    "                txt = filter(None, txt.split(di))\n",
    "                txt = ('\\n'+di+'\\n').join(txt)\n",
    "            # fix figure and table captions\n",
    "            txt = re.sub(r'(?s)(Figure\\s[0-9]{1,2}\\n*?)\\[.*?\\]\\n\\n', '\\g<1>', txt)\n",
    "\n",
    "        elif jnl == 'BIOLOGICAL PSYCHIATRY':\n",
    "            # fix inline citations\n",
    "            txt = re.sub(r'(?s)\\n*?\\[([0-9].*?)\\].*?See\\sall\\sReferences', '(\\g<1>)', txt)\n",
    "            txt = re.sub(r'(?s)\\(+[0-9]{1,3}\\)+\\(*([0-9]{1,3})\\)*', '(\\g<1>)', txt)\n",
    "            # remove inline table of contents\n",
    "            txt = re.sub(r'(?s)Jump\\sto\\sSection.*?(#|\\Z)', '\\g<1>', txt)\n",
    "            # fix figure caption duplications\n",
    "            cpt = 'Figure\\s[0-9]{1,2}[A-Z]{,1}'\n",
    "            txt = re.sub(r'(?s)(\\['+cpt+'\\])'+cpt, '\\g<1>', txt)\n",
    "            cpt = '(?:Figures{,1}\\sS|Tables{,1}\\sS|Supplement\\s)[0-9]{1,2}[A-Z]{,1}';\n",
    "            txt = re.sub(r'(?s)('+cpt+')'+cpt, '\\g<1>', txt)\n",
    "\n",
    "        elif jnl == 'NEURON':\n",
    "            # fix inline citations\n",
    "            yyyy = '[12][90][0-9][0-9][a-z]{,1}'\n",
    "            txt = re.sub(r'(?s)(\\({,1}'+yyyy+'\\){,1})\\n\\n.{,1000}?\\[Google\\sScholar\\]', '\\g<1> ', txt)\n",
    "            # fix figure caption display\n",
    "            txt = re.sub(r'(?s)(\\n\\n(?:Figure|Table)\\s[0-9]{1,2})([A-Za-z]{2,})', '\\g<1>. \\g<2>', txt)\n",
    "            txt = re.sub(r'(\\[Download \\(PPT\\)\\]).*?\\n', '\\g<1>', txt)\n",
    "\n",
    "        elif jnl == 'PLOS BIOLOGY':\n",
    "            # fix image urls\n",
    "            txt = txt.replace('<img src=\"article/', '<img src=\"https://journals.plos.org/plosbiology/article/')\n",
    "\n",
    "        elif jnl == 'NATURE':\n",
    "            # filter all text after Editorial Summary\n",
    "            txt = txt.split('\\n# Editorial Summary')\n",
    "            assert(len(txt) <= 2)\n",
    "            if len(txt) == 2:\n",
    "                txt[1] = txt[1].replace('\\n## ', '\\n# ')\n",
    "            txt = '\\n## Editorial Summary'.join(txt)\n",
    "\n",
    "        # clean up space and escape asterisks\n",
    "        txt = re.sub(r'(?s)\\s+(\\]|\\))', '\\g<1>', txt)\n",
    "        txt = re.sub(r'(?s)(\\[|\\()\\s+', '\\g<1>', txt)\n",
    "        txt = re.sub(r'(?s)(\\]|\\))\\s+(\\[|\\()', '\\g<1> \\g<2>', txt)\n",
    "        txt = re.sub(r'(?s)\\s+(\\.|:|;|,)', '\\g<1>', txt)\n",
    "        txt = re.sub(r'(?s)(;|,)\\s+', '\\g<1> ', txt)\n",
    "        txt = re.sub(r'(?s)(\\n\\n)\\n+', '\\g<1>', txt)\n",
    "        txt = txt.replace('*', '\\*')\n",
    "\n",
    "        recs[i]['TXT'] = txt\n",
    "\n",
    "    print('\\n\\nArticle formatting cleaned.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5. Filter sections, highlight relevant terms, and download equation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if processing_complete:\n",
    "    display(iw.VBox([processing_complete_message], layout=align_center))\n",
    "\n",
    "else:    \n",
    "    # set of article sections to be removed\n",
    "    null = {r'ABOUT THIS ARTICLE', r'ABSTRACT', r'ACCESSION NUMBERS', r'ACCESSIONS', \n",
    "            r'ACKNOWLEDGEMENT', r'ACKNOWLEDGEMENTS', r'ACKNOWLEDGEMENTS AND DISCLOSURES', \n",
    "            r'ACKNOWLEDGMENTS', r'ACKNOWLEDGMENTS AND DISCLOSURES', r'ADDITIONAL INFORMATION', \n",
    "            r'AFFILIATIONS', r'APPENDIX A SUPPLEMENTARY MATERIAL', \n",
    "            r'APPENDIX A SUPPLEMENTARY MATERIALS', r'APPENDIX A SUPPLEMENTARY METERIALS', \n",
    "            r'APPENDIX A SUPPORTING INFORMATION', r'ARTICLE AND AUTHOR INFORMATION', \n",
    "            r'ARTICLE INFO', r'ARTICLE OUTLINE', r'ARTICLE RELATED AUDIO', \n",
    "            r'ARTICLE RELATED VIDEO', r'ARTICLE TOOLS', r'ARTICLES', r'AUTHOR CONTRIBUTIONS', \n",
    "            r'AUTHOR INFORMATION', r'AUTHOR NOTES', r'AUTHOR RESPONSE', r'AUTHOR SUMMARY', \n",
    "            r'AUTHORS', r'AVAILABILITY OF DATA AND CODE', \n",
    "            r'BE THE FIRST TO READ NEW ARTICLES FROM ELIFE', r'CHANGE HISTORY', \n",
    "            r'CITATION MANAGER FORMATS', r'CODE AVAILABILITY', r'COMMENTS', \n",
    "            r'COMPETING INTERESTS', r'CONCLUSION', r'CONCLUSIONS', r'CONFLICT OF INTEREST', \n",
    "            r'CONSORTIA', r'COPYRIGHT NOTICE', r'CORRESPONDING AUTHOR', \n",
    "            r'CORRESPONDING AUTHORS', r'CSV FILES', r'DATA AVAILABILITY', r'DECISION LETTER', \n",
    "            r'DISCUSSION', r'DOWNLOAD LINKS', r'EDITORIAL SUMMARY', \n",
    "            r'ELECTRONIC SUPPLEMENTARY MATERIAL', r'ELIFE DIGEST', r'EXCEL FILES', \n",
    "            r'EXTENDED DATA', r'EXTENDED DATA FIGURES', r'EXTENDED DATA FIGURES AND TABLES', \n",
    "            r'EXTENDED DATA TABLES', r'FIGURES', r'FOOTNOTES', r'FUNDING', r'FURTHER READING', \n",
    "            r'HIGHLIGHTS', r'INFORMATION', r'INTEGRATED SUPPLEMENTARY INFORMATION', \n",
    "            r'INTRODUCTION', r'JUMP TO SECTION', r'KEYWORDS', r'LINKED ARTICLES', r'MAIN', \n",
    "            r'MAIN MENU', r'METRICS', r'NATURE', r'NATURE COMMUNICATIONS', \n",
    "            r'NATURE COMMUNICATIONS MENU', r'NATURE MENU', r'NATURE NEUROSCIENCE', \n",
    "            r'NATURE NEUROSCIENCE MENU', r'NATURECOM SITEMAP', r'NEW RESEARCH IN', \n",
    "            r'NMHS SOURCE CODE', r'NOT PERMITTED', r'PDF FILES', r'PERMITTED', r'PNAS PORTALS', \n",
    "            r'REFERENCES', r'REFERENCES AND NOTES', r'RELATED ARTICLES', \n",
    "            r'RIGHTS AND PERMISSIONS', r'SEARCH', r'SI DISCUSSION', \n",
    "            r'SIGN UP FOR ARTICLE ALERTS', r'SIGNIFICANCE', r'SOURCE DATA', r'SUMMARY', \n",
    "            r'SUMMARY AND CONCLUSIONS', r'SUPPLEMENTAL INFORMATION', r'SUPPLEMENTARY DATA', \n",
    "            r'SUPPLEMENTARY FIGURES', r'SUPPLEMENTARY INFORMATION', r'SUPPLEMENTARY MATERIAL', \n",
    "            r'SUPPLEMENTARY MATERIALS', r'SUPPORTING INFORMATION', r'USER MENU', r'VIDEOS', \n",
    "            r'YOU MAY ALSO BE INTERESTED IN', r'ZIP FILES', r'[SCIENCE]', \n",
    "            r'[SHOW]ARTICLE INFO'}\n",
    "\n",
    "    term = {r'centralit', r'clubs{,1}\\b', r'cluster[ie]', r'communit', r'controllab', r'cores{,1}\\b', \n",
    "            r'degree', r'distan', r'divers', r'efficien', r'flexib', r'graph', r'hub', r'modular', \n",
    "            r'module', r'motif', r'participat', r'paths{,1}\\b', r'strength', r'topolog', r'world'}\n",
    "    \n",
    "    for i, ri in enumerate(recs):\n",
    "        # get journal name and article text\n",
    "        print('Article %d/%d'%(i+1, len(recs)))\n",
    "        jnl = ri['SO']\n",
    "        sec = ri['TXT']\n",
    "        acc = ri['UT']\n",
    "\n",
    "        # download equations and modify markdown code\n",
    "        for ui in re.findall(r'(?s)<img.*?src=\"(.*?)\"', sec.split('## ', 1)[1]):\n",
    "            if len(re.findall(r'(?s)^/\\w', ui)):\n",
    "                continue\n",
    "            url = re.sub(r'(?s)^(https{,1}:){,1}/*', 'http://', ui)\n",
    "            eqn = os.path.join(eqns_dir, re.sub(r'(?s)(\\W|_)+', '_', 'ref%s_'%acc+url))\n",
    "            if not os.path.isfile(eqn):\n",
    "                print('Saving eqn: '+url)\n",
    "                get = requests.get(url, headers=hdr, verify=False)\n",
    "                with open(eqn, 'wb') as f:\n",
    "                    f.write(get.content)\n",
    "            sec = re.sub(r'(?s)(<img.*?src=)\"'+re.escape(ui)+'\"', '\\g<1>\"'+repr(eqn)[1:-1]+'\"', sec)\n",
    "\n",
    "        # disable autolinks\n",
    "        sec = sec.replace('http://', 'h<span>tt</span>p://')\n",
    "        sec = sec.replace('https://', 'h<span>tt</span>ps://')\n",
    "        sec = sec.replace('www.', 'w<span>ww</span>.')\n",
    "        \n",
    "        # split text by paragraphs\n",
    "        sec = re.sub(' *\\n *', '\\n', sec).split('\\n\\n')\n",
    "        sec = [pi.strip() for pi in sec]\n",
    "        \n",
    "        # highlight terms in pink\n",
    "        spanbgc = lambda i: '<span style=\"background-color: '+('lightyellow' if i else 'pink')+'\">'\n",
    "        fnd = r'(?is)([\\s\\-](?:'+'|'.join(term)+')[a-zA-Z\\-]*)'\n",
    "        rpl = spanbgc(0)+'\\g<1></span>'\n",
    "        for j, pi in enumerate(sec):\n",
    "            # exclude headings\n",
    "            if not pi.startswith('#'):\n",
    "                # protect math\n",
    "                if '<math>' in pi:\n",
    "                    txt = pi.split('<math>')\n",
    "                    t0 = txt.pop(0)\n",
    "                    txtm, txt = zip(*(ti.split('</math>') for ti in txt))\n",
    "                    txt = [re.sub(fnd, rpl, ' '+ti) for ti in [t0] + list(txt)]\n",
    "                    txtm = ['<math>'+ti+'</math>' for ti in txtm]\n",
    "                    sec[j] = ''.join(txt[:1] + [''.join(ti) for ti in zip(txtm, txt[1:])])\n",
    "                elif '$$' in pi:\n",
    "                    txt = pi.split('$$')\n",
    "                    if len(txt) % 2:\n",
    "                        txt, txtm = txt[0::2], txt[1::2]\n",
    "                        txt = [re.sub(fnd, rpl, ' '+ti) for ti in txt]\n",
    "                        txtm = ['$$'+re.sub(r'\\\\+', r'\\\\', ti)+'$$' for ti in txtm]\n",
    "                        sec[j] = ''.join(txt[:1] + [''.join(ti) for ti in zip(txtm, txt[1:])])\n",
    "                    else:\n",
    "                        sec[j] = re.sub(fnd, rpl, ' '+pi)\n",
    "                else:\n",
    "                    sec[j] = re.sub(fnd, rpl, ' '+pi)\n",
    "        # highlight paragraphs in yellow\n",
    "        sec = [spanbgc(1)+pi+'</span>' if spanbgc(0) in pi else pi for pi in sec]\n",
    "        \n",
    "        # split text by sections and clean up\n",
    "        sec = ['<<<'+pi+'>>>' if pi.startswith('## ') else pi for pi in sec]\n",
    "        sec = ('\\n\\n'.join(sec)).split('<<<')[1:]\n",
    "        sec = [si.split('>>>') for si in sec]\n",
    "        sec = [[re.sub(r'[0-9#:\\.!\\\\\\n]', '', si[0]).strip().upper(), si[1]] for si in sec]\n",
    "\n",
    "        # include the main section of nature journals papers\n",
    "        # for short papers that have no results sections\n",
    "        main = {''}\n",
    "        if 'NATURE' in jnl:\n",
    "            head = set(list(zip(*sec))[0]) - null\n",
    "            if (len(head) <= 2) and ('RESULTS' not in head):\n",
    "                main = {'MAIN'}\n",
    "\n",
    "        # filter and store article text\n",
    "        tru = [si for si in sec if not si[0] in (null - main)]\n",
    "        tru = '\\n\\n'.join(['## '+si[0]+'\\n\\n'+si[1] for si in tru])        \n",
    "        recs[i]['TRU'] = tru\n",
    "        \n",
    "    print('\\n\\nArticle text filtered and key terms highlighted.')\n",
    "\n",
    "    # # optionally print the null and term sets\n",
    "    # for h in [null, term]:\n",
    "    #     ci = 0\n",
    "    #     print('{', end='')\n",
    "    #     for i, ni in enumerate(sorted(h)):\n",
    "    #         ci += len(ni) + 4\n",
    "    #         if ci > 80:\n",
    "    #             print('\\n            ', end='')\n",
    "    #             ci = len(ni) + 4\n",
    "    #         print('r\\'%s\\''%ni, end=', ' if i+1 < len(h) else '}')\n",
    "    #     print('\\n\\n')\n",
    "\n",
    "    # # optionally print frequency of terms\n",
    "    # word = {ki: 0 for ki in term}\n",
    "    # for ri in recs:\n",
    "    #     for wi in re.findall(r'(?s)'+re.escape(spanbgc(0))+'.*?</span>', ri['TRU']):\n",
    "    #         for ki in word:\n",
    "    #             word[ki] += (1 if re.findall('(?is)'+ki, wi) else 0)\n",
    "\n",
    "    # k, v = zip(*word.items())\n",
    "    # for vi, ki in sorted(zip(v, k))[::-1]:\n",
    "    #     print('% 5d %s'%(vi, ki))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6. Display and evaluate processed article text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # optionally filter articles by type:\n",
    "# evidence_circularity = 'yes'\n",
    "# recs = [ri for ri in recs if ri['Q4']==evidence_circularity]\n",
    "\n",
    "# # optionally display only highlighted paragraphs\n",
    "# for i, ri in enumerate(recs):\n",
    "#     recs[i]['TRU'] = '\\n\\n'.join([pi for pi in ri['TRU'].split('\\n\\n') if pi.strip().startswith('<')])\n",
    "\n",
    "if not processing_complete:\n",
    "    # shuffle article order and start with an example\n",
    "    random.shuffle(recs)\n",
    "    recs.insert(0, recs.pop([i for i, ri in enumerate(recs) if ri['UT']=='000389746000001'][0]))\n",
    "    \n",
    "    # specify that processing is now complete\n",
    "    processing_complete = 1\n",
    "\n",
    "# define constants\n",
    "n = len(recs)\n",
    "h = tkinter.Tk().winfo_screenheight()\n",
    "\n",
    "# store value function\n",
    "def store_value(button):\n",
    "    # index of record\n",
    "    i = paper_field.value - 1\n",
    "    \n",
    "    # store value of pressed button\n",
    "    ki = button['owner'].description\n",
    "    vi = button['owner'].value\n",
    "    recs[i][ki] = vi\n",
    "    \n",
    "    # get button index and adjust other values\n",
    "    di = int(ki[1]) - 1\n",
    "    if di < 2:\n",
    "        iwa[di+1].value = 'not answered'\n",
    "    # set bottom decision\n",
    "    if (di == 1 and vi != 'yes') or di == 2:\n",
    "        iwa[3].value = vi\n",
    "    # specify final assessment\n",
    "    if di == 0:\n",
    "        if vi == 'no':\n",
    "            iwa[3].value = 'not applicable'\n",
    "        else:\n",
    "            iwa[3].value = 'not answered'\n",
    "    # disable or enable buttons   \n",
    "    if di < 2:\n",
    "        iwa[di+1].disabled = vi != 'yes'\n",
    "        \n",
    "# change paper with button\n",
    "def button_press(button):\n",
    "    if button.description == 'Previous article':\n",
    "        paper_field.value = max(paper_field.value-1, 1)\n",
    "    elif button.description == 'Next article':\n",
    "        paper_field.value = min(paper_field.value+1, n)\n",
    "\n",
    "# change paper with inpaper_field\n",
    "def change_paper(value):\n",
    "    ri = recs[value['new']-1]\n",
    "    # change text\n",
    "    text.clear_output()\n",
    "    sec = '\\n\\n'.join(['`Article '+ri['UT']+'`', '`'+ri['AB'].replace(\"`\", \"'\")+'`', ri['TRU']])\n",
    "    text.append_display_data(Markdown(sec))\n",
    "    \n",
    "    # restore or reset evaluations\n",
    "    for i, ai in enumerate(iwa):\n",
    "        ki = ai.description\n",
    "        if ki in ri:\n",
    "            iwa[i].value = ri[ki]\n",
    "        else:\n",
    "            iwa[i].value = 'not answered'\n",
    "        \n",
    "    # reset tabs and set button status\n",
    "    tabs.selected_index = 0\n",
    "    prev_button.disabled = paper_field.value == 1\n",
    "    next_button.disabled = paper_field.value == n\n",
    "\n",
    "# change viewing box height\n",
    "def change_height(value):\n",
    "    height = str(round(h * value['new'] / 100))\n",
    "    text.layout.height = height+'px'\n",
    "    evln.layout.height = height+'px'\n",
    "\n",
    "# create widgets\n",
    "prev_button = iw.Button(description='Previous article')\n",
    "next_button = iw.Button(description='Next article')\n",
    "paper_field = iw.BoundedIntText(description='Article:',\n",
    "                                value=n, min=1, max=n, step=1,\n",
    "                                layout=iw.Layout(width='150px'),\n",
    "                                style={'description_width': 'initial'},\n",
    "                                continuous_update=False)\n",
    "height_slider = iw.IntSlider(description='Text-window height:',\n",
    "                           value=100, min=30, max=100, step=1, \n",
    "                           style={'description_width': 'initial'},\n",
    "                           continuous_update=True)\n",
    "\n",
    "menu = iw.HBox([prev_button, next_button, paper_field, height_slider],\n",
    "            layout=iw.Layout(justify_content='space-around'))\n",
    "text = iw.Output()\n",
    "\n",
    "# define widget behavior\n",
    "prev_button.on_click(button_press)\n",
    "next_button.on_click(button_press)\n",
    "paper_field.observe(change_paper, names='value')\n",
    "height_slider.observe(change_height, names='value')\n",
    "\n",
    "# create evaluation widget and define behavior\n",
    "hsub = lambda M: tuple('<i>'+m[0]+'</i><sub>'+m[1]+'</sub></i>' for m in M)\n",
    "\n",
    "ques = [('<ol start=\"1\"><li>'\n",
    "         'Presence of at least one network-neuroscience model.'\n",
    "         '</li></ol>'),\n",
    "        ('<hr><ol start=\"2\"><li>'\n",
    "         'Acceptance of at least one %s, where:'\n",
    "         '<ul>'\n",
    "         '<li>%s is a network-neuroscience model of the studied data.</li>'\n",
    "         '<li>%s includes a feature %s that represents some function %s.</li>'\n",
    "         '<li>There is no strong known mechanistic link between %s and %s.</li>'\n",
    "         '</ul>'\n",
    "         '</li></ol>')%hsub(('M1', 'M1', 'M1', 'X1', 'F1', 'X1', 'F1')),\n",
    "        ('<hr><ol start=\"3\"><li>'\n",
    "         'No test of %s against at least one %s, where:'\n",
    "         '<ul>'\n",
    "         '<li>%s is a model of the same studied data.</li>'\n",
    "         '<li>%s includes only features with known mechanistic links to function.</li>'\n",
    "         '<li>%s is known, or likely to explain, %s as a redundant feature.</li>'\n",
    "         '</ul>'\n",
    "         '</li></ol>')%hsub(('M1', 'M0', 'M0', 'M0', 'M0', 'X1')),\n",
    "        ('<hr><ol start=\"4\"><li>Evidence of circular network-neuroscience analysis.</li></ol>')]\n",
    "answ = ['not answered', 'no', 'yes', 'unclear', 'not applicable']\n",
    "\n",
    "iwa = []; iwq = [];\n",
    "for i, qi in enumerate(ques):\n",
    "    iwq.append(iw.HTML(value=qi))\n",
    "    iwa.append(iw.ToggleButtons(options=answ[:i+(3 if i < 2 else 2)],\n",
    "                                description='Q%d'%(i+1),\n",
    "                                style={'description_width': '0px'}))\n",
    "    if i:\n",
    "        iwa[-1].disabled = True\n",
    "    iwa[-1].observe(store_value, names='value')\n",
    "\n",
    "evln = iw.VBox(      [iwq[0], iwa[0],\n",
    "                      iwq[1], iwa[1],\n",
    "                      iwq[2], iwa[2],\n",
    "                      iwq[3], iwa[3]])\n",
    "\n",
    "text.append_display_data(Markdown(' '))\n",
    "\n",
    "tabs = iw.Tab(children=[text, evln])\n",
    "tabs.set_title(0, 'Methods and Results')\n",
    "tabs.set_title(1, 'Evaluation')\n",
    "\n",
    "paper_field.value = 1\n",
    "height_slider.value = 70\n",
    "display(menu)\n",
    "display(tabs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7. Save results to a database file and a summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_info(button):\n",
    "    if button is not None:\n",
    "        # create and write to the archive file (8 specifies deflation)\n",
    "        keys = ['UT', 'AB', 'TRU'] + [ai.description for ai in iwa]\n",
    "        filt_dict = lambda ri: {ki: vi for (ki, vi) in ri.items() if ki in keys}\n",
    "        try:\n",
    "            with zipfile.ZipFile(results_file, 'w', 8) as zf:\n",
    "                zf.writestr('records.json', json.dumps([filt_dict(ri) for ri in recs]))\n",
    "                for fi in os.listdir(eqns_dir):\n",
    "                    zf.write(os.path.join(eqns_dir, fi), fi)\n",
    "            print(results_file+' saved;', end=' ')        \n",
    "        except PermissionError:\n",
    "            print(results_file+' not saved: permission denied;', end=' ')\n",
    "        try:\n",
    "            with open(summary_file, 'w') as f:\n",
    "                f.write('Accession Number,'+','.join([ai.description for ai in iwa])+'\\n')\n",
    "                for ri in recs:\n",
    "                    f.write(ri['UT']+',')\n",
    "                    for ai in iwa:\n",
    "                        ki = ai.description\n",
    "                        if ki in ri:\n",
    "                            f.write(ri[ki]+',')\n",
    "                        else:\n",
    "                            f.write('not answered,')\n",
    "                    f.write('\\n')\n",
    "                print(summary_file+' saved.', end=' ')\n",
    "        except PermissionError:\n",
    "            print(summary_file+' not saved: permission denied.', end=' ')\n",
    "        print('\\n', end='')\n",
    "\n",
    "menu_messag = iw.HTML('''\n",
    "                    <div class=\"alert alert-block alert-warning\" style=\"font-weight: 600\">\n",
    "                    Save results to %s<br>\n",
    "                    NB: this may overwrite existing files.</div>\n",
    "                    '''%os.path.abspath(out_dir))\n",
    "save_button = iw.Button(description='Save results')\n",
    "save_button.on_click(save_info)\n",
    "display(iw.VBox([menu_messag, save_button], layout=align_center))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
